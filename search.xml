<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FSub-millisecond-Stateful-Stream-Querying-over-Fast-evolving-Linked-Data%2F</url>
      <content type="text"><![CDATA[ABSTRACT场景：社交网络，城市监控和市场馈送处理等应用需要有状态的流式查询，状态流查询不仅要查询流式数据，还要查询存储的数据来及时提取有用的信息。实时流数据提供的有用信息，也需要持续不断地整合到存储的数据中，以便为上述和未来提供查询服务。然而，先前的流式处理系统或者侧重于流计算，或者不是有状态的，或者不能提供低延迟和高吞吐量来处理快速发展的Linked数据，并且能够支持不断增加的查询并发性。Wukong + S采用集成设计，将流处理和持久化存储相结合，实现高效的状态共享，避免了传统复合设计（如Storm / Heron + Wukong）中的跨系统成本和次优查询性能。 Wukong + S使用混合存储来区分管理持久的数据和瞬时数据，并提供有效的流索引和本地分区，以便快速访问流数据。 Wukong + S进一步提供分散的矢量时间戳和有界的快照标量化，以节省内存使用量的节点和大量查询。 INTRODUCTION：问题提出场景：随着流数据和存储数据量的不断增加，及时查询有用的信息十分重要。对于公共数据集合数据流，可能有大量的用户不同的数据流查询请求，因此需要支持高并发的查询。而且流数据通常包含巨大的有用信息， 这样的数据应该被一致地和立即地整合到存储系统，以用于将来的连续查询。现有的工作：然而目前现有的系统对于正在变化的数据集的侧重点在于流计算。流计算和流查询不同的是 前者通常倾向于对大部分流数据进行序列化计算，而后者侧重于对流和存储数据的特定集合的并发查询。大多数先前的系统也没有集成流数据为了并发的查询中，或者不查询持久化存储的历史数据来获得基础知识，因此是无状态的。 尽管大多数流处理数据库都明确支持语义和SQL接口，但是他们在快速演化的Linked 数据下，当面临大量并发的查询请求下，由于高昂的Join操作开销和一些ACID的语义，他们的查询性能是低效的。 解决方案： Wukong-S为了尊重数据本地化并最大限度地减少数据传输，它使用由基于时间的临时存储和连续持久化存储组成的混合存储，为正在到来的数据和持久化的数据提供不同的存储管理。 Wukong + S提供了流数据的快速访问流索引。流数据通过局部感知分区进行分片，其中一些流索引在节点间动态复制。这节省了查询成本并提供高效的负载平衡。 为了在多个不同规模的流数据上提供一致的流查询，Wukong + S使用分散的矢量时间戳来推导出最近一致性状态的流式数据插入。Wukong + S使用有限的标量化方案将矢量时间戳投影到标量快照数量中，通过协调多个流的更新到底层持久存储区。这样的设计在有效的内存使用情况下扩展了Wukong + S节点和大量查询。 MOTIVATION根据工作负载特性，提出了一条流旨在支持大量的查询系统(连续的和一次性的)对流和存储的查询数据。有几种独特的需求可以区分来自其他流系统的流查询系统。 APPROACH ANN OVERVIEW 目前存在的系统采用的是讲流处理系统和以查询为主导的存储系统简单结合。简单的组合设计会造成性能低效的结果。比如讲Esper和Apahe Jena组合 Issue1. Cross-system Cost Issue2. Sub-optimal query plan Issue3. Limited scalability Wukong+S做的就是一个整体设计。Wukong+ S使用了一个内置的设计，目标是对流和存储数据进行连续和一次性的查询。关键的设计原则就是对待持久化的存储作为关键的解决方案]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bipartite_graph 二分图]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FBipartite-graph-%E4%BA%8C%E5%88%86%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[（bigraph）是有两个相互独立的位置图和连接图构成。二分图的概念是由图灵奖获得者Milner提出的，其目的为普适计算提供统一的元模型。 若无向图G = 的结点集V能够划分为两个子集V1,V2，满足V1∩V2 = F(空集)，且V1∪V2 = V（全集），使得G中任意一条边的两个端点，一个属于V1，另一个属于V2，则称G为偶图（Bipartite Graph）或二分图（Bigraph）。V1和V2称为互补结点子集，偶图也可记为G = 。 二分图，二部图，偶图，是图论中一种特殊模型。指顶点可以分成两个不相交的集使得在同一个集内的顶点不相邻（没有共同边）的图。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RDMA技术详解]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FRDMA%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[面对高性能计算、大数据分析和浪涌型IO高并发、低时延应用，现有TCP/IP软硬件架构和应用高CPU消耗的技术特征根本不能满足应用的需求。这要有体现在处理延时过大，数十微秒；多次内存拷贝、中断处理，上下文切换、复杂的TCP/IP协议处理、网络延时过大、存储转发模式和丢包导致额外延时。接下来我们继续讨论RDMA技术、原理和优势，看完文章你就会找到为什么RDMA可以更好的解决这一系列问题。 DMA是一种远端内存直接访问技术，详细介绍请参看文章。RDMA最早专属于Infiniband架构，随着在网络融合大趋势下出现的RoCE和iWARP，这使高速、超低延时、极低CPU使用率的RDMA得以部署在目前使用最广泛的以太网上。 RDMAC（RDMA Consortium）和IBTA（InfiniBand Trade Association）主导了RDMA发展，RDMAC是IETF的一个补充并主要定义的是iWRAP和iSER，IBTA是infiniband的全部标准制定者，并补充了RoCE v1 v2的标准化。IBTA解释了RDMA传输过程中应具备的特性行为，而传输相关的Verbs接口和数据结构原型是由另一个组织OFA（Open Fabric Alliance）来完成。 相比传统DMA的内部总线IO，RDMA通过网络在两个端点的应用软件之间实现Buffer的直接传递；相比比传统的网络传输，RDMA又无需操作系统和协议栈的介入。RDMA可以轻易实现端点间的超低延时、超高吞吐量传输，而且基本不需要CPU、OS等资源介入，也不必再为网络数据的处理和搬移耗费过多其他资源。 InfiniBand通过以下技术保证网络转发的低时延（亚微秒级），采用Cut-Through转发模式，减少转发时延；基于Credit的流控机制，保证无丢包；硬件卸载；Buffer尽可能小，减少报文被缓冲的时延 。 iWARP(RDMA over TCP/IP) 利用成熟的IP网络；继承RDMA的优点；TCP/IP硬件实现成本高，但如果采用传统IP网络丢包对性能影响大。 RoCE性能与IB网络相当；DCB特性保证无丢包；需要以太网支持DCB特性；以太交换机时延比IB交换机时延要稍高一些。 RoCEv2针对RoCE进行了一些改进，如引入IP解决扩展性问题，可以跨二层组网；引入UDP解决ECMP负载分担等问题。 基于InfiniBand的RDMA是在2000年发布规范，属于原生RDMA；基于TCP/IP的RDMA称作iWARP，在 2007年形成标准，主要包括MPA/DDP/RDMAP三层子协议；基于Ethernet的RDMA叫做RoCE，在2010年发布协议，基于增强型以太网并将传输层换成IB传输层实现。 扩展RDMA API接口以兼容现有协议/应用，OFED(Open Fabrics Enterprise Distribution)协议栈由OpenFabric联盟发布，分为Linux和windows版本，可以无缝兼容已有应用。通过使已有应用与RDMA结合后，性能成倍提升。 应用和RNIC（RDMA-aware network interface controller）之间的传输接口层（Software Transport Interface）被称为Verbs。OFA(Open Fabric Alliance)提供了RDMA传输的一系列Verbs API。OFA开发了OFED（Open Fabric Enterprise Distribution）协议栈，支持多种RDMA传输层协议。 OFED向下除了提供RNIC(实现 RDMA 和LLP( Lower Layer Protocol))基本的队列消息服务外，向上还提供了ULP（Upper Layer Protocols），通过ULP上层应用不需直接和Verbs API对接，而是借助于ULP与应用对接，这样使得常见的应用不需要做修改就可以跑在RDMA传输层上。 在Infiniband/RDMA的模型中，核心是如何实现应用之间最简单、高效和直接的通信。RDMA提供了基于消息队列的点对点通信，每个应用都可以直接获取自己的消息，无需操作系统和协议栈的介入。 消息服务建立在通信双方本端和远端应用之间创建的Channel-IO连接之上。当应用需要通信时，就会创建一条Channel连接，每条Channel的首尾端点是两对Queue Pairs（QP），每对QP由Send Queue（SQ）和Receive Queue（RQ）构成，这些队列中管理着各种类型的消息。QP会被映射到应用的虚拟地址空间，使得应用直接通过它访问RNIC网卡。除了QP描述的两种基本队列之外，RDMA还提供一种队列Complete Queue（CQ），CQ用来知会用户WQ上的消息已经被处理完。 RDMA提供了一套软件传输接口，方便用户创建传输请求Work Request(WR），WR中描述了应用希望传输到Channel对端的消息内容，WR通知QP中的某个队列Work Queue（WQ）。在WQ中，用户的WR被转化为Work Queue Ellement（WQE）的格式，等待RNIC的异步调度解析，并从WQE指向的Buffer中拿到真正的消息发送到Channel对端。 RDMA中SEND/RECEIVE是双边操作，即必须要远端的应用感知参与才能完成收发。READ和WRITE是单边操作，只需要本端明确信息的源和目的地址，远端应用不必感知此次通信，数据的读或写都通过RDMA在RNIC与应用Buffer之间完成，再由远端RNIC封装成消息返回到本端。在实际中，SEND/RECEIVE多用于连接控制类报文，而数据报文多是通过READ/WRITE来完成的。 对于双边操作为例，主机A向主机B(下面简称A、B)发送数据的流程如下： 首先，A和B都要创建并初始化好各自的QP，CQ A和B分别向自己的WQ中注册WQE，对于A，WQ=SQ，WQE描述指向一个等到被发送的数据；对于B，WQ=RQ，WQE描述指向一块用于存储数据的Buffer。 A的RNIC异步调度轮到A的WQE，解析到这是一个SEND消息，从Buffer中直接向B发出数据。数据流到达B的RNIC后，B的WQE被消耗，并把数据直接存储到WQE指向的存储位置。 AB通信完成后，A的CQ中会产生一个完成消息CQE表示发送完成。与此同时，B的CQ中也会产生一个完成消息表示接收完成。每个WQ中WQE的处理完成都会产生一个CQE。双边操作与传统网络的底层Buffer Pool类似，收发双方的参与过程并无差别，区别在零拷贝、Kernel Bypass，实际上对于RDMA，这是一种复杂的消息传输模式，多用于传输短的控制消息。 对于单边操作，以存储网络环境下的存储为例(A作为文件系统，B作为存储介质)，数据的流程如下（RDMA READ）： 首先A、B建立连接，QP已经创建并且初始化。 数据被存档在A的buffer地址VA，注意VA应该提前注册到A的RNIC，并拿到返回的local key，相当于RDMA操作这块buffer的权限。 A把数据地址VA，key封装到专用的报文传送到B，这相当于A把数据buffer的操作权交给了B。同时A在它的WQ中注册进一个WR，以用于接收数据传输的B返回的状态。 B在收到A的送过来的数据VA和R_key后，RNIC会把它们连同存储地址VB到封装RDMA READ，这个过程A、B两端不需要任何软件参与，就可以将A的数据存储到B的VB虚拟地址。 B在存储完成后，会向A返回整个数据传输的状态信息。 单边操作传输方式是RDMA与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用的参与其中，这种方式适用于批量数据传输。 Infiniband的成功取决于两个因素，一是主机侧采用RDMA技术，可以把主机内数据处理的时延从几十微秒降低到几微秒，同时不占用CPU；二是InfiniBand网络的采用高带宽（40G/56G）、低时延（几百纳秒）和无丢包特性 随着以太网的发展，也具备高带宽和无丢包能力，在时延方面也能接近InfiniBand交换机的性能，所以RDMA over Ethernet（RoCE）成为必然，且RoCE组网成本更低。未来RoCE、iWARP和Infiniband等基于RDMA技术产品都会得到长足的发展。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[内存映射IO (MMIO)]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2F%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84IO-MMIO%2F</url>
      <content type="text"><![CDATA[MMIO(Memory mapping I/O)即内存映射I/O，它是PCI规范的一部分，I/O设备被放置在内存空间而不是I/O空间。从处理器的角度看，内存映射I/O后系统设备访问起来和内存一样。这样访问AGP/PCI-E显卡上的帧缓存，BIOS，PCI设备就可以使用读写内存一样的汇编指令完成，简化了程序设计的难度和接口的复杂性。 基本概念MMIO(Memory mapping I/O)即内存映射I/O，它是PCI规范的一部分，I/O设备被放置在内存空间而不是I/O空间。从处理器的角度看，内存映射I/O后系统设备访问起来和内存一样。这样访问AGP/PCI-E显卡上的帧缓存，BIOS，PCI设备就可以使用读写内存一样的汇编指令完成，简化了程序设计的难度和接口的复杂性。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O)。（来自百度百科）简而言之，MMIO就是通过将外围设备映射到内存空间，便于CPU的访问。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O)。前者就是我们常说的I/O端口，它实际上的应该被称为I/O地址空间。小概念：32位操作系统，32bit的处理器，拥有32bit寻址能力，即可访问2^32=4G的物理地址，那么就具有4G内存的识别能力。物理地址：并不是指物理内存的地址，而是指处理器和系统内存之间所用到的地址，可以理解为CPU最为方便访问的地址（有别于我们之前所知道的物理地址的定义：段地址*16+偏移地址），而这一个内存并不独属于物理内存，而被分成了很多部分，物理内存当然也能够占用其中的一部分。 PortIO和MMIO 的主要区别1）前者不占用CPU的物理地址空间，后者占有（这是对x86架构说的，一些架构，如IA64，port I/O占用物理地址空间）。2）前者是顺序访问。也就是说在一条I/O指令完成前，下一条指令不会执行。例如通过Port I/O对设备发起了操作，造成了设备寄存器状态变化，这个变化在下一条指令执行前生效。uncache的MMIO通过uncahce memory的特性保证顺序性。3）使用方式不同由于port I/O有独立的64K I/O地址空间，但CPU的地址线只有一套，所以必须区分地址属于物理地址空间还是I/O地址空间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCIE和PCI 计算机总线]]></title>
      <url>%2Fblog%2F2018%2F05%2F23%2FPCIE%E5%92%8CPCI-%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%80%BB%E7%BA%BF%2F</url>
      <content type="text"><![CDATA[PCIEPCI Express是新一代的总线接口。早在2001年的春季，英特尔公司就提出了要用新一代的技术取代PCI总线和多种芯片的内部连接，并称之为第三代I/O总线技术。随后在2001年底，包括Intel、AMD、DELL、IBM在内的20多家业界主导公司开始起草新技术的规范，并在2002年完成，对其正式命名为PCI Express。它采用了目前业内流行的点对点串行连接，比起PCI以及更早期的计算机总线的共享并行架构，每个设备都有自己的专用连接，不需要向整个总线请求带宽，而且可以把数据传输率提高到一个很高的频率，达到PCI所不能提供的高带宽。 基本概念PCI Express的接口根据总线位宽不同而有所差异，包括X1、X4、X8以及X16（X2模式将用于内部接口而非插槽模式）。较短的PCI Express卡可以插入较长的PCI Express插槽中使用。PCI Express接口能够支持热拔插，这也是个不小的飞跃。PCI Express卡支持的三种电压分别为+3.3V、3.3Vaux以及+12V。用于取代AGP接口的PCI Express接口位宽为X16，将能够提供5GB/s的带宽，即便有编码上的损耗但仍能够提供4GB/s左右的实际带宽，远远超过AGP 8X的2.1GB/s的带宽。 PCI Express规格从1条通道连接到32条通道连接，有非常强的伸缩性，以满足不同系统设备对数据传输带宽不同的需求。例如，PCI Express X1规格支持双向数据传输，每向数据传输带宽250MB/s，PCI Express X1已经可以满足主流声效芯片、网卡芯片和存储设备对数据传输带宽的需求，但是远远无法满足图形芯片对数据传输带宽的需求。因此，必须采用PCI Express X16，即16条点对点数据传输通道连接来取代传统的AGP总线。PCI Express X16也支持双向数据传输，每向数据传输带宽高达4GB/s，双向数据传输带宽有8GB/s之多，相比之下，广泛采用的AGP 8X数据传输只提供2.1GB/s的数据传输带宽。 尽管PCI Express技术规格允许实现X1（250MB/秒），X2，X4，X8，X12，X16和X32通道规格，但是依形式来看，PCI Express X1和PCI Express X16将成为PCI Express主流规格，同时芯片组厂商将在南桥芯片当中添加对PCI Express X1的支持，在北桥芯片当中添加对PCI Express X16的支持。除去提供极高数据传输带宽之外，PCI Express因为采用串行数据包方式传递数据，所以PCI Express接口每个针脚可以获得比传统I/O标准更多的带宽，这样就可以降低PCI Express设备生产成本和体积。另外，PCI Express也支持高阶电源管理，支持热插拔，支持数据同步传输，为优先传输数据进行带宽优化。 在兼容性方面，PCI Express在软件层面上兼容的PCI技术和设备，支持PCI设备和内存模组的初始化，也就是说驱动程序、操作系统无需推倒重来，就可以支持PCI Express设备。PCI Express是新一代能够提供大量带宽和丰富功能以实现令人激动的新式图形应用的全新架构。PCI Express可以为带宽渴求型应用分配相应的带宽，大幅提高中央处理器（CPU）和图形处理器（GPU）之间的带宽。对最终用户而言，他们可以感受影院级图象效果，并获得无缝多媒体体验。 PCI Express的主要优势就是数据传输速率高，目前最高的16X 2.0版本可达到10GB/s，而且还有相当大的发展潜力。PCI Express也有多种规格，从PCI Express 1X到PCI Express 16X，能满足一定时间内出现的低速设备和高速设备的需求。PCI-Express最新的接口是PCIe 3.0接口，其比特率为8GT/s，约为上一代产品带宽的两倍，并且包含发射器和接收器均衡、PLL改善以及时钟数据恢复等一系列重要的新功能，用以改善数据传输和数据保护性能。像INTEL、IBM、、LSI、OCZ、、三星(计划中)、SanDisk、STEC、SuperTalent和东芝(计划中)等，而针对海量的数据增长使得用户对规模更大、可扩展性更强的系统所应用，PCIe 3.0技术的加入最新的LSI MegaRAID控制器及HBA产品的出色性能，就可以实现更大的系统设计灵活性。 PCI Express采用串行方式传输Data。它和原有的ISA、PCI和AGP总线不同。这种传输方式，不必因为某个硬件的频率而影响到整个系统性能的发挥。当然了，整个系统依然是一个整体，但是我们可以方便的提高某一频率低的硬件的频率，以便系统在没有瓶颈的环境下使用。以串行方式提升频率增进效能，关键的限制在于采用什么样的物理传输介质。人们普遍采用铜线路，而理论上铜这个材质可以提供的传输极限是10 Gbps。这也就是为什么PCI Express的极限传输速度的答案。 因为PCI Express工作模式是一种称之为“电压差式传输”的方式。两条铜线，通过相互间的电压差来表示逻辑符号0和1。以这种方式进行资料传输，可以支持极高的运行频率。所以在速度达到10Gbps后，只需换用光纤（Fibre Channel）就可以使之效能倍增。 PCI Express是下一阶段的主要传输总线带宽技术。然而，GPU对总线带宽的需求是子系统中最高的，显而易见的是，视频在PCI Express应占有一定的分量。显然，PCI Express的提出，并非是总线形式的一个结束。恰恰相反，其技术的成熟仍旧需要这个时间。当然了，趁这个时间，那些芯片、主板、视频等厂家是否能出来支持是PCI Express发展的关键。 PCI-Express是最新的总线和接口标准，它原来的名称为“3GIO”，是由英特尔提出的，很明显英特尔的意思是它代表着下一代I/O接口标准。交由PCI-SIG（PCI特殊兴趣组织）认证发布后才改名为“PCI-Express”。这个新标准将全面取代现行的PCI和AGP，最终实现总线标准的统一。它的主要优势就是数据传输速率高，目前最高可达到10GB/s以上，而且还有相当大的发展潜力。PCI Express也有多种规格，从PCI Express 1X到PCI Express 16X，芯片组。当然要实现全面取代PCI和AGP也需要一个相当长的过程，就象当初PCI取代ISA一样，都会有个过渡的过程。 PCI和PCIE有什么区别 在兼容性方面，PCI-E在软件层面上兼容目前的PCI技术和设备，支持PCI设备和内存模组的初始。 由于PCI总线只有133MB/s 的带宽，对声卡、网卡、视频卡等绝大多数输入/输出设备显得绰绰有余，但对性能日益强大的显卡则无法满足其需求。 目前PCI接口的显卡已经不多见了，只有较老的PC上才有，厂商也很少推出此类接口的产品。PCI显卡性能受到极大限制，并且由于数量稀少，因此价格也并不便宜，只有在不得已的情况才考虑使用PCI显卡。 因此，用于取代AGP接口的PCI-E接口位宽为X16，能够提供5GB/s的带宽，即便有编码上的损耗但仍能够提供约为4GB/s左右的实际带宽，远远超过AGP 8X的2.1GB/s的带宽。 PCI(Peripheral Component Interconnect)外部设备互连总线是 英特尔（Intel）公司1991年下半年首先提出的，并得到IBM、Compad、AST、HP、和DEC等100多家计算机公司的响应，于1993年正式推出了PCI局部总线标准。此标准允许在计算机内安装多达10个遵从PCI标准的扩展卡。 PCI-Express是最新的总线和接口标准，它原来的名称为“3GIO”，是由英特尔提出的，很明显英特尔的意思是它代表着下一代I/O接口标准。交由PCI-SIG（PCI特殊兴趣组织）认证发布后才改名为“PCI-Express”，简称“PCI-E”。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Design Guidelines for High Performance RDMA Systems]]></title>
      <url>%2Fblog%2F2018%2F05%2F23%2FDesign-Guidelines-for-High-Performance-RDMA-Systems%2F</url>
      <content type="text"><![CDATA[Introduction随着RDMA技术的发展，RDMA技术越来越被数据中心采用。尽管RDMA的新知名度，使用他们的先进功能以达到最佳效果仍然是软件设计师的挑战。找到RDMA功能与应用程序之间的有效匹配非常重要。没有一种方法能够适合所有的应用场景，比如说RDMA一个参数的最佳和最差选择在它们的总吞吐量中变化了70倍，并且它们消耗的主机CPU的量变化了3.2倍。在不同的设计中，应用需求的小的变化显著影响RDMA的相对性能。 首先这篇文章的第一个贡献就是它提供了由一组开源的度量工具支持的指导方针，用于评估和优化在使用RDMA NICs时影响端到端吞吐量的最重要的系统因素。对于每个指导方针。作者就如何确定本指南是否相关提供了深入的见解，并讨论了使用NIC的哪些模式最有可能缓解问题。 其次，作者通过第三代RDMA硬件将这些指南应用于微基准和实际系统来评估这些指南的功效。 Background图显示了RDMA Cluster硬件组件。其中NIC网卡连接的一个或则多个端口连接到PCIe控制器且连接到多核CPU的服务器上。PCIe恐控制器用来接受NIC网卡的PCIe请求到L3 cache中。在现代的Intel 服务器架构中，这个L3 Cache 提供PCIe 事件控制器。 PCI Express]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2Fblog%2F2017%2F09%2F25%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux服务管理]]></title>
      <url>%2Fblog%2F2017%2F04%2F25%2FLinux%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
      <content type="text"><![CDATA[Linux服务器管理第一章简介与分类1-1系统运行级别 0关机 1单用户模式 2不完全的命令行模式 3完全的命令行模式 4系统保留 5图像模式 6重启动 修改系统默认允许级别方法 vim /etc/inittab系统开机后直接进入哪个运行级别 1-2服务的分类 rpm包默认安装的服务 独立的服务 基于xinetd的服务 源码包安装的服务 1-3启动与自启动 服务启动：就是在当前系统中让服务运行，并且提供功能 服务自启动：自启动就是指在系统开机或者重启动以后，随着系统启动而自启动服务。 1-4查看系统中启动的服务 RPM包安装的服务 chkconfig –list 查看服务启动状态，可以看到所有RPM包安装的服务.RPM包安装位置在/etc/init/init.d 目录下 源码包安装的服务 查看服务安装的位置一般在 /usr/local 目录下 1-5 服务和端口 端口和服务相对应 查询系统中开启的服务 netstat -tlunp 第二章RPM包服务管理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo 强大的博客框架]]></title>
      <url>%2Fblog%2F2017%2F04%2F25%2FHexo-%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%A1%86%E6%9E%B6%2F</url>
      <content type="text"><![CDATA[正文: 什么是Hexo Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装 安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我会尽力解决您的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello]]></title>
      <url>%2Fblog%2F2015%2F07%2F01%2Fhello-blog%2F</url>
      <content type="text"><![CDATA[摘要:正文:]]></content>
    </entry>

    
  
  
</search>
