<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Fast and Concurrent RDF Queries with RDMA-based Distributed Graph Exploration]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FUntitled%2F</url>
      <content type="text"><![CDATA[RDF图Graph应用场景：通过对大量且不断增长的RDF数据进行大量查询，RDF图形存储库为并发查询处理提供低延迟和高吞吐量势在必行。 现有的工作存在的问题：然而，先前的系统在大数据集上仍然经历高的查询延迟，并且大多数先前的设计具有较差的资源利用率，使得每个查询被顺序地处理。查询处理集中的依赖于潜在大表的连接操作，这通常会产生巨大的冗余中间数据。 此外，使用关系表triplets来存储三元组可能会限制一般性，使得现有系统难以支持RDF数据的一般图形查询，如可达性分析和社区检测。 现有的解决方案： 使用triple 存储和 triple join方法存在的问题：First,使用三元组存储会过度依赖Join操作，特别是分布式环境下的merge/hash join操作。Second, scan-join操作会产生大量的中间冗余结果。Third, 尽管现有的工作使用redundant six primary SPO4 permutation index 可以加速join操作，但是索引会导致大量的内存开销。 使用Graph store 和 Graph exploration存在的问题：之前的工作表明，最后一步join相应的子查询结果会造成一个潜在的性能瓶颈。特别是查询那些存在环的语句，或者有很大的中间结果的情况下。‘ Graph Model And Graph Indexs在Wukong中这里有两种不同类型的索引结构。分别是 Predicate Index和Type Index索引。 Wukong提出了预测索引（P-idx）来维护所有使用其特定谓词标记的主体和对象入边和出边。索引顶点本质上充当从谓词到相应的主体或对象的倒排索引。Wukong还提出了一种Type Index索引方便查询一个Subject属于的类型。与先前基于图的方法（使用单独的数据结构管理索引）不同，Wukong将索引作为RDF图的基本部分（顶点和边）处理，同时还考虑了这些索引的分割和存储。好处：首先，这使用图探索简化了查询处理，以便图探索可以直接从索引顶点开始。 其次，这使得在多个服务器之间分配索引变得简单而高效。 Differentiated Graph Partitioning受到PowerLyra的启发，Wukong采用不同的分区策略算法对于正常顶点和索引顶点来说。每个正常顶点（例如，DS）将被随机分配（即，通过 散列顶点ID）到只有一个机器的所有边缘（邻居的ID）。与正常顶点不同的是，每个索引顶点（例如，takeCourse和Course）将被拆分并复制到多个机器，其边缘链接到同一机器上的正常顶点。 这很自然地将索引和它们的负载分配给每台机器。 RDMA-friendly Predicate-based StoreWukong采用一种基于RDMA-Based的分布式hash表结构存储RDF Graph Data。在这样的结构中，它包含两种不同的索引结构，一种是Type-index索引，存储Subject/Objetc的类型索引。一种是Predicate-Index索引，存储的是谓词的相邻顶点的索引。 Query Processing Query Basic Query ProcessingWukong利用图探索通过沿着图特别是根据子图的每个边。对于大多数情况下(谓词通常是知道的恒定变量，然而subject/object是自由变量)，Wukong利用谓词索引开始进行图探索。对于那些查询是一个子图环的查询，三个Subjet/Object都是自由变量。Wukong根据基于cost的方法和一些启发式的选择一个探索顺序。对于一些罕见的情况，那些谓词都是不知道的情况下，Wukong从一个静态的(常量)的顶点进行图形探索（通过pred 已知的顶点相关联的谓词）。 Full-history Pruning在探索查询的每一个阶段中，通过RDMA READ读取其他机器上的数据，进行裁剪。裁剪那些没有必要的冗余数据。 Migrating Execution or Data对于一个查询阶段，如果有很少的顶点数据需要抓取从远程机器中，Wukong 使用一个本地执行的模式同步利用单边RDMA READ直接从远程顶点抓取数据。对于一个查询阶段，如果许多顶点需要被抓取。Wuong 利用一个Fork-Join 执行模式异步的分开查询计算到多个子查询在远程机器上。 Concurrent Query ProcessingWork-obliger work 窃取算法邻近的Worker进程的查询超时时间（s.end &lt; now）。如果是这样的话这个Worker可能在处理冗长的查询，因此后续的查询任务可能被延迟。在这种情况下，这个Worker从该Worker的工作对队列中窃取一个查询任务来处理。在逼迫相邻的woker(知道看到一个不忙的Worker)，Worker 进程持续通过从其中自己的工作队列中，持续处理自己的查询。持续处理自己的查询。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[GraphX Graph Processing in a Distributed Dataflow Framework]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2F2014-OSDI-GraphX-Graph-Processing-in-a-Distributed-Dataflow-Framework%2F</url>
      <content type="text"><![CDATA[解决的问题：虽然现有的专用图系统能够实现广泛的系统优化，但也是有代价的。 图只是较大的分析过程的一部分，通常将非结构化的图形和表格式数据组合在一起。 因此，分析流水线（例如图11）被迫组成多个系统，这增加了复杂性并导致不必要的数据移动和重复。 此外，为了追求性能，图形处理系统通常会放弃容错，以支持快照恢复。 最后，作为专门的图处理系统，图处理框架通常不能享受分布式数据流框架的广泛支持。 相比之下，通用分布式数据流框架（例如Map-Reduce [10]，Spark [39]，Dryad [15]）暴露丰富的数据流操作符（例如map，reduce，group-by，join） 用于分析非结构化和表格化数据，并被广泛采用。 但是，直接使用数据流操作符来实现迭代图算法可能具有挑战性，往往需要复杂连接的多个阶段。 此外，分布式数据流框架中定义的通用连接和聚合策略不利用迭代图算法中的常见模式和结构，因此错过了重要的优化机会。 为了支持这个论点，我们引入了GraphX，一个嵌入到Spark [39]分布式数据流系统中的高效的图形处理框架。 它首先对每个Triplet三元组视图的每个元素进行一个MapFunction操作，得到一个消息。这个消息包括，属性数据以及一个目标顶点的ID。就是说你要将这个消息发送到哪个顶点上去。Message Combiners通过累加这些消息，然后将累加结构更新到相应顶点的属性数据。GraphX同这样一个GAS的思想，进行图形迭代计算。这个思想很像MapReduce的思想。通过这样的操作，不停的进行迭代计算。最终图形中每个顶点的属性数据收敛不再变化。整个图形算法最终完成。 3.3 GraphX operatorsGraphX Pregel abstraction: 它的主要思想是通过构造出这样一个Triplet 三元组视图这样的一个结构。这个结构是通过将顶点RDD和边RDD进行Join操作得到 这样一个三元组信息。为什么要得到这样一个Triplet 三元组视图这样的一个结构。最主要的原因是因为这个三元组视图包括源顶点属性、目标顶点属性、以及边属性。这样我们就可以通过一些类似于专业图处理系统的GAS计算模型进行 类似MpaReduce思想的图形计算。 它首先对每个Triplet三元组视图的每个元素进行一个MapFunction操作，得到一个消息。这个消息包括，属性数据以及一个目标顶点的ID。就是说你要将这个消息发送到哪个顶点上去。Message Combiners通过累加这些消息，然后将累加结构更新到相应顶点的属性数据。GraphX同这样一个GAS的思想，进行图形迭代计算。这个思想很像MapReduce的思想。通过这样的操作，不停的进行迭代计算。最终图形中每个顶点的属性数据收敛不再变化。整个图形算法最终完成。 GraphX优化Vertex Mirroring由于GraphX的顶点和边属性集合分区都是独立的，然而Join操作需要数据移动。GraphX则通过将顶点属性通过网络传输到边分区上。由于两个原因，这种方法大大减少了通信。 首先，现实世界的图通常比顶点具有更多数量级的边。 其次，单个顶点在同一个分区中可能有许多边，从而实现顶点属性的大量重用。 Multicast Join(多路广播)虽然所有顶点被发送到每个边缘分区的广播连接将确保连接发生在边缘分区上，但是由于大多数分区只需要一小部分顶点来完成连接，所以它仍然是低效的。 因此，GraphX引入了多播连接，其中每个顶点属性仅发送到包含相邻边的边缘分区。 Partial Materialization （部分实体化）当顶点属性改变时，顶点复制被急切地执行，但是，在边缘分区上的本地join没有实现，以避免重复。 相反，镜像顶点属性存储在每个边界分区上的散列映射中，并在构建三元组时引用。 Incremental View Maintenacne当下一次访问三元组视图时，只有已更改的顶点被重新路由到其边缘分割连接点，并且未更改的顶点的局部Mirror被重用。然后被更改的顶点更新相应的Triplets View视图。 mrTriplets 优化4.3.1 Filtered Index ScanningmrTriplets操作的第一阶段就是通过扫描triplets三元组视图，然后应用用户定义的map 函数应用到每个triplets三元组中。然而随着迭代算法的进行，工作集合被收缩，大多数顶点已经收敛。Map function只需要操作那些活跃顶点的triplets三元组视图。直接顺序扫描所有的索引将会变得很浪费。 为了解决这个问题，GraphX提出了Index Scanning对于triplets三元组视图。也就是说应用程序通过使用subgraph运算符来限制图来表示当前活跃顶点。活跃顶点通过route table (查询活跃顶点属于相应的edge partitions)被推送edge partitions，它可以用来使用源顶点id上的CSR索引来快速查找到相应的edge（4.1节）。 4.3.2 Automatic Join Elimination问题：对于triplets三元组视图的某些operator访问来说，可能某些顶点属性没有访问，或则根本一个都没有使用。那么对于triplets三元组视图来说，会造成一部分资源浪费，因为riplets三元组视图被构造的时候所有的source vertex属性和distinction vertex属性都被join构造到triplets视图中。 GraphX使用JVM字节码分析器在运行时检查用户定义的函数，并确定是否引用源或目标顶点属性。如果只引用一个属性，并且三元组视图尚未实现，则GraphX自动重写生成的查询计划 三联视图从三联加入到双向联接。 如果没有引用顶点属性，则GraphX完全消除连接。 这种修改是可能的，因为三元组视图遵循Spark中RDD的惰性语义。 如果用户从不访问三元组视图，则永远不会实现。 因此，对mrTriplets的调用能够重写生成三元组视图的相关部分所需的联接。 GraphX 容错Graph实现容错主要是通过丢失分区的剩余副本或者重新计算它们来恢复]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[TUX2: Distributed Graph Computation for Machine Learning]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FTUX2-Distributed-Graph-Computation-for-Machine-Learning%2F</url>
      <content type="text"><![CDATA[Introduce在图形引擎（如GraphLab [29]）上的早期工作是基于机器学习的动机，基于观察到许多机器学习问题可以用图形自然而有效地建模，并通过迭代收敛算法解决。 问题：然而，大多数后续的图形引擎工作都采用简单的图计算模型，由PageRank等基本图形基准测试驱动。 由此产生的图形引擎缺乏高效分布式机器学习的灵活性和其他关键功能。 Heterogeneous vertices：在机器学习中，顶点都有不同的属性。然而在图计算中，图引擎一般都只含有一个属性的顶点。这样对于机器学习算法来说，会带来更多的编程复杂性和性能低下。 Mini-Batch：Mini-Batch是一个十分重要的概念在机器学习中。但是在图计算中缺没有这样的概念，Min-Batch意味着在机器学习中，训练一部分子集然后更新整个模型。 Flexible consistency在分布式图处理系统中，多个Worker一起工作处理整个图。在每次迭代的结束，图引擎会有一个很严重的Barrier限制。然而在Min-Bach中，Hard Barrier在多个Mini-Batch中会造成太多的同步开销。由于机器算法可以容忍这样的状态。所以它们使用这样一种Soft Barrier的机制 进行局部同步。 作者提出TUX2，一个分布式图形引擎，用于在图模型中表示的机器学习算法。 TUX2保留了图计算的好处，同时还支持过时同步并行（SSP）模型[20,11,42,13]，异构数据模型和新的MEGA（小批量，Exchange，GlobalSync和 应用）图形模型以实现高效的分布式机器学习. TuX2 DesginTUX2旨在保留图形引擎的优势，同时将其数据模型，编程模型和调度方法扩展到分布式机器学习。 TUX2使用顶点切割方法(vertex-cut)，其中（高度）顶点的边集可以分成多个分区，每个分区保持顶点的副本。其中一个副本被指定为Master顶点;它维护顶点数据的主版本。所有剩余的副本都称为Mirror顶点，并且每个副本都维护一个本地缓存副本。我们采用顶点切割是因为它在处理幂律图中被证明是有效的，并且它自然地连接到参数服务器模型（paraemter-server model）[26,11]：所有顶点数据的主版本可以被视为（分布式）全局状态存储在参数服务器中。在每个分区中，TUX2将顶点和边保持在单独的数组中。Edge数组中的Edge按source vertex分组。每个顶点都有一个索引，给出它在Edge数组中的边集的偏移量。每条边都包含诸如包含目标顶点的分区的ID和相应顶点数组中该顶点的索引等信息。该图形数据结构适用于遍历，并且使用查找表优于顶点索引。 每个分区都由一个进程管理，该进程在逻辑上同时扮演一个Worker角色，计算分区中的顶点并沿着Edge传播顶点数据，以及一个Server角色，以同步镜像顶点与其相对应Master顶点之间的状态。 在进程内部，TUX2使用多个线程进行并行化，并将分区的Server角色和Worker角色分配给同一个线程。 然后每个线程负责计算用于本地计算的Mirrors顶点子集，并维护进程拥有的分区中Master顶点子集的状态。 图2显示了数据在TUX2中如何分区，存储和分配给执行角色。 Heterogenous Data Layout虽然传统图形引擎简单地假设了一个同质图，但TUX2支持多种数据布局维度的异构性，包括顶点类型和分区方法; 它甚至支持主点和镜像顶点数据类型之间的异构性。 Tux2重点介绍二分图上的优化，因为许多机器学习问题自然地映射为具有两个不相交顶点集的二部图，例如MF中的用户和项目，LR中的特征和样本等等。 因此这两组顶点通常具有不同的属性。 例如，在LR的情况下，只有特征顶点包含权重字段，并且只有样本顶点包含目标标签字段。 而且，在像BlockPG [27]这样的LR变体中，特征顶点也维护着过去的历史信息。因此，TUX2允许用户定义不同的顶点类型，并将不同类型的顶点放置在不同的数组中。 这导致紧凑的数据表示，从而改善计算过程中的数据局部性。此外不同的顶点类型可能包含不同的顶点度数。例如，在用户项目图中，项目顶点可以链接到数千个用户，但用户顶点通常仅链接到数十个项目。TUX2使用PowerLyra [7]和BiGraph [8]中提出的二分图感知分区算法，以便只有高度顶点才具有Mirror 顶点。 在二分图中，TUX2可以通过扫描一个类型的顶点来列举所有的边。列举边类型的选择有时具有显著的性能影响。用Mini-Batch来扫描镜像顶点会导致更有效的同步步骤，只要TUX2能够识别有更新的镜像集合，这些镜像会与它们的Master同步，因为这些顶点被连续地放置在数组中。相比之下，如果TUX2在一个Mini-Batch中扫描没有镜像的顶点，那么在扫描过程中为其他顶点类型更新的镜像将被分散，从而更昂贵的定位。因此，TUX2允许用户指定在计算过程中需要计算的顶点集。 Figure3 显示了如何在二分图下组织顶点数据，并以用户-项目图中的MF为例。由于用户顶点的度数一般都比较小，所有只有项目顶点使用点划分方法切割顶点。因此，Server角色中的Master顶点数组只包含项目顶点，而worker角色只管理用户顶点。这样，用户顶点也就没有镜像副本，也不需要分布式同步。在工作者角色中，项目和用户顶点的镜像存储在两个独立的数组中。 另一种类型的异质性来自顶点的主副本副本上执行的不同计算，这可能需要不同的数据结构以实现同步效率。 例如，BlockPG算法访问和更新小批量中的一组特征的权重，而在采样顶点计算的目标函数可能取决于不在该块中的特征的权重。 这导致镜像上的辅助特征顶点属性，以记录特征权重的历史增量以递增地计算目标函数的值。 但是，主属性上不需要此增量属性，因此不需要在同步期间进行交换。 同样，主顶点也维护一些镜像上不需要的额外属性。 因此，TUX2允许用户为同一个顶点的主副镜像定义不同的数据结构。 Scheduling with SSPTUX2支持过时的同步并行(SSP)模型[11]，有有界的过时和小批量。SSP是基于每个clock的概念，其中一个clock对应于一个mini-Batch的迭代，并且由一组并发任务完成。迭代批处理可以看作是一个特殊的情况，每个迭代都使用所有的输入数据。SSP引入了一个显式的松弛参数，它指定了一个clock全局共享状态的视图的停顿。因此，这一空缺决定了任何任务可能取得进展的最慢的任务。随着s的松弛，时钟t上的任务保证会看到从时钟1到t-s-的所有更新，它可能会看到从时钟t-s到t-1的更新。图4展示了一个SSP执行，其松弛度为1。 TUX2在具有指定大小的Mini-Batch上执行每个迭代。每个工作人员首先选择一组顶点或边作为当前的小批量执行。在小批处理完成后，TUX2通过继续列举顶点或边缘数组的连续段，获取下一个小批量的另一组顶点或边。TUX2在小批处理粒度中支持SSP。它跟踪每个小批量迭代的进度，以便计算时钟。如果在所有的工作人员上完成相应的小批处理(包括主和镜像之间的同步)，并且如果结果更新被应用到并在状态中反映出来，那么工作人员就会认为时钟t已经完成了。一个工人可以执行一个任务在时钟t只有它知道所有时钟t -s -1已经完成,其中s是允许松懈。 MEGA Model in TUX2 TUX2TUX2引入了一个新的基于Stage的MEGA模型，其中每个阶段是对一组顶点及其Edge的计算。 每个阶段都有用户定义的函数（UDF）应用于在其中访问的顶点或边。 TUX2支持四种类型的阶段：Mini-Batch，Exchange，GlobalSync和Apply（因此命名为MEGA）; 它允许用户构建任意阶段的顺序。 该引擎负责调度每个阶段中多个CPU core或机器上的UDF并行执行。 MEGA模型保留了GAS模型的简单性，同时引入了更多的灵活性来解决支持机器学习算法的GAS模型的缺陷。 例如，在诸如MF和LDA的算法中，处理一条边涉及更新两个顶点。 这需要两个GAS阶段，但是可以在我们的模型的一个Exchange阶段中完成。 对于LR，两个方向上的顶点数据传播之后应该跟一个Apply阶段，但是不需要Scatter阶段; 这可以在MEGA模型中避免，因为MEGA允许任意阶段的顺序。 接下来我们详细阐述不同类型的阶段。 Exchange Exchange（）在每个列举的边上执行。 Du和Dv分别是顶点u和v的数据。 D（u，v）是与边缘（u，v）相关的数据。 au，av和a（u，v）是顶点数据和边缘数据的相应累积增量，τ是与每个工作线程相关联的用户定义的共享上下文，并在整个计算过程中保持不变。 所有这些参数都允许在此UDF中更新。 用户可以使用它为顶点和边缘生成新的积累变化量，或者直接更新它们的状态。由于点划分的切割策略，Exchange（）只能更新顶点的镜像版本数据（即局部状态）。用户也可以使用τ来计算和存储一些算法特定的非图上下文数据，这可能会 通过全球聚合共享。 默认情况下，未为枚举指定的顶点受顶点级锁保护，但TUX2还允许用户为某些应用程序实现其自己的无锁语义[14,21,37]。 这个阶段比GAS模型中的聚集/散布阶段更灵活，因为它不暗示或强制顶点数据沿着边缘传播的方向，并且它可以更新同一UDF中两个顶点的状态。 从而提高LDA和MF等算法的效率。 Apply 这个阶段枚举一组顶点并同步它们的Master版本和Mirror版本。 对于每个顶点，Master从Mirror中累积增量，调用Apply（Du，au，τ）来更新其全局状态，然后更新Mirror上的状态。 为了支持Master和Mirror之间的异构，TUX2允许用户为需要同步的顶点的全局状态定义基类VertexDataSync; Master和Mirror可以定义不同的子类，每个子类继承自基类，以包含其他信息。 引擎仅同步Master顶点和Mirror顶点之间的VertexDataSync中的数据。 GlobalSync 该阶段负责同步跨作业者线程的上下文和/或通过一组顶点聚合数据。 有三个UDF与这个阶段相关联:Aggregate（）将跨顶点的数据聚合到Workers上下文τ中。 Combine（）将Worker的上下文τ聚合到一个特殊的Worker中，该Worker为不同的时钟维护多个版本的上下文τ以支持SSP。 Apply（）完成全局聚合的τ（例如，用于重新缩放）。 执行Apply（）后，最终的聚合τ会同步回所有Workers。 如果未提供Aggregate（）函数，则此阶段将仅在Worker中汇总和同步上下文τ。 Mini-Batch 这是一个包含一系列其他阶段的复合阶段; 它定义了每个小批量迭代执行的阶段。 MiniBatch根据要在每个最小批次中枚举的版本或边的数量来定义最小批量大小，对于双边图，则列出要枚举的顶点类型（请参见§4中的示例）。 Conclusion通过TUX2，我们提倡图计算和分布式机器学习的融合。 TUX2代表了朝这个方向迈出的关键一步，不仅展示了这种融合的可行性，而且展示了这种融合的潜力。 我们通过将重要的机器学习概念引入图计算来实现这一点; 定义一个新的灵活的图模型来有效地表达机器学习算法; 并通过对代表性机器学习算法进行广泛的评估来证明其优点。 展望未来，我们希望TUX2将为进一步研究图计算和分布式机器学习提供一个共同的基础，从而允许更多的机器学习算法和优化能够被轻松高效地表达和实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Sub-millisecond Stateful Stream Querying over Fast-evolving Linked Data]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FSub-millisecond-Stateful-Stream-Querying-over-Fast-evolving-Linked-Data%2F</url>
      <content type="text"><![CDATA[ABSTRACT场景：社交网络，城市监控和市场馈送处理等应用需要有状态的流式查询，状态流查询不仅要查询流式数据，还要查询存储的数据来及时提取有用的信息。实时流数据提供的有用信息，也需要持续不断地整合到存储的数据中，以便为上述和未来提供查询服务。然而，先前的流式处理系统或者侧重于流计算，或者不是有状态的，或者不能提供低延迟和高吞吐量来处理快速发展的Linked数据，并且能够支持不断增加的查询并发性。Wukong + S采用集成设计，将流处理和持久化存储相结合，实现高效的状态共享，避免了传统复合设计（如Storm / Heron + Wukong）中的跨系统成本和次优查询性能。 Wukong + S使用混合存储来区分管理持久的数据和瞬时数据，并提供有效的流索引和本地分区，以便快速访问流数据。 Wukong + S进一步提供分散的矢量时间戳和有界的快照标量化，以节省内存使用量的节点和大量查询。 INTRODUCTION：问题提出场景：随着流数据和存储数据量的不断增加，及时查询有用的信息十分重要。对于公共数据集合数据流，可能有大量的用户不同的数据流查询请求，因此需要支持高并发的查询。而且流数据通常包含巨大的有用信息， 这样的数据应该被一致地和立即地整合到存储系统，以用于将来的连续查询。现有的工作：然而目前现有的系统对于正在变化的数据集的侧重点在于流计算。流计算和流查询不同的是 前者通常倾向于对大部分流数据进行序列化计算，而后者侧重于对流和存储数据的特定集合的并发查询。大多数先前的系统也没有集成流数据为了并发的查询中，或者不查询持久化存储的历史数据来获得基础知识，因此是无状态的。 尽管大多数流处理数据库都明确支持语义和SQL接口，但是他们在快速演化的Linked 数据下，当面临大量并发的查询请求下，由于高昂的Join操作开销和一些ACID的语义，他们的查询性能是低效的。 解决方案： Wukong-S为了尊重数据本地化并最大限度地减少数据传输，它使用由基于时间的临时存储和连续持久化存储组成的混合存储，为正在到来的数据和持久化的数据提供不同的存储管理。 Wukong + S提供了流数据的快速访问流索引。流数据通过局部感知分区进行分片，其中一些流索引在节点间动态复制。这节省了查询成本并提供高效的负载平衡。 为了在多个不同规模的流数据上提供一致的流查询，Wukong + S使用分散的矢量时间戳来推导出最近一致性状态的流式数据插入。Wukong + S使用有限的标量化方案将矢量时间戳投影到标量快照数量中，通过协调多个流的更新到底层持久存储区。这样的设计在有效的内存使用情况下扩展了Wukong + S节点和大量查询。 MOTIVATION根据工作负载特性，提出了一条流旨在支持大量的查询系统(连续的和一次性的)对流和存储的查询数据。有几种独特的需求可以区分来自其他流系统的流查询系统。 APPROACH ANN OVERVIEW 目前存在的系统采用的是讲流处理系统和以查询为主导的存储系统简单结合。简单的组合设计会造成性能低效的结果。比如讲Esper和Apahe Jena组合 Issue1. Cross-system Cost Issue2. Sub-optimal query plan Issue3. Limited scalability Wukong+S做的就是一个整体设计。Wukong+ S使用了一个内置的设计，目标是对流和存储数据进行连续和一次性的查询。关键的设计原则就是对待持久化的存储作为关键的解决方案]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Bipartite_graph 二分图]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FBipartite-graph-%E4%BA%8C%E5%88%86%E5%9B%BE%2F</url>
      <content type="text"><![CDATA[（bigraph）是有两个相互独立的位置图和连接图构成。二分图的概念是由图灵奖获得者Milner提出的，其目的为普适计算提供统一的元模型。 若无向图G = 的结点集V能够划分为两个子集V1,V2，满足V1∩V2 = F(空集)，且V1∪V2 = V（全集），使得G中任意一条边的两个端点，一个属于V1，另一个属于V2，则称G为偶图（Bipartite Graph）或二分图（Bigraph）。V1和V2称为互补结点子集，偶图也可记为G = 。 二分图，二部图，偶图，是图论中一种特殊模型。指顶点可以分成两个不相交的集使得在同一个集内的顶点不相邻（没有共同边）的图。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[RDMA技术详解]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2FRDMA%E6%8A%80%E6%9C%AF%E8%AF%A6%E8%A7%A3%2F</url>
      <content type="text"><![CDATA[面对高性能计算、大数据分析和浪涌型IO高并发、低时延应用，现有TCP/IP软硬件架构和应用高CPU消耗的技术特征根本不能满足应用的需求。这要有体现在处理延时过大，数十微秒；多次内存拷贝、中断处理，上下文切换、复杂的TCP/IP协议处理、网络延时过大、存储转发模式和丢包导致额外延时。接下来我们继续讨论RDMA技术、原理和优势，看完文章你就会找到为什么RDMA可以更好的解决这一系列问题。 DMA是一种远端内存直接访问技术，详细介绍请参看文章。RDMA最早专属于Infiniband架构，随着在网络融合大趋势下出现的RoCE和iWARP，这使高速、超低延时、极低CPU使用率的RDMA得以部署在目前使用最广泛的以太网上。 RDMAC（RDMA Consortium）和IBTA（InfiniBand Trade Association）主导了RDMA发展，RDMAC是IETF的一个补充并主要定义的是iWRAP和iSER，IBTA是infiniband的全部标准制定者，并补充了RoCE v1 v2的标准化。IBTA解释了RDMA传输过程中应具备的特性行为，而传输相关的Verbs接口和数据结构原型是由另一个组织OFA（Open Fabric Alliance）来完成。 相比传统DMA的内部总线IO，RDMA通过网络在两个端点的应用软件之间实现Buffer的直接传递；相比比传统的网络传输，RDMA又无需操作系统和协议栈的介入。RDMA可以轻易实现端点间的超低延时、超高吞吐量传输，而且基本不需要CPU、OS等资源介入，也不必再为网络数据的处理和搬移耗费过多其他资源。 InfiniBand通过以下技术保证网络转发的低时延（亚微秒级），采用Cut-Through转发模式，减少转发时延；基于Credit的流控机制，保证无丢包；硬件卸载；Buffer尽可能小，减少报文被缓冲的时延 。 iWARP(RDMA over TCP/IP) 利用成熟的IP网络；继承RDMA的优点；TCP/IP硬件实现成本高，但如果采用传统IP网络丢包对性能影响大。 RoCE性能与IB网络相当；DCB特性保证无丢包；需要以太网支持DCB特性；以太交换机时延比IB交换机时延要稍高一些。 RoCEv2针对RoCE进行了一些改进，如引入IP解决扩展性问题，可以跨二层组网；引入UDP解决ECMP负载分担等问题。 基于InfiniBand的RDMA是在2000年发布规范，属于原生RDMA；基于TCP/IP的RDMA称作iWARP，在 2007年形成标准，主要包括MPA/DDP/RDMAP三层子协议；基于Ethernet的RDMA叫做RoCE，在2010年发布协议，基于增强型以太网并将传输层换成IB传输层实现。 扩展RDMA API接口以兼容现有协议/应用，OFED(Open Fabrics Enterprise Distribution)协议栈由OpenFabric联盟发布，分为Linux和windows版本，可以无缝兼容已有应用。通过使已有应用与RDMA结合后，性能成倍提升。 应用和RNIC（RDMA-aware network interface controller）之间的传输接口层（Software Transport Interface）被称为Verbs。OFA(Open Fabric Alliance)提供了RDMA传输的一系列Verbs API。OFA开发了OFED（Open Fabric Enterprise Distribution）协议栈，支持多种RDMA传输层协议。 OFED向下除了提供RNIC(实现 RDMA 和LLP( Lower Layer Protocol))基本的队列消息服务外，向上还提供了ULP（Upper Layer Protocols），通过ULP上层应用不需直接和Verbs API对接，而是借助于ULP与应用对接，这样使得常见的应用不需要做修改就可以跑在RDMA传输层上。 在Infiniband/RDMA的模型中，核心是如何实现应用之间最简单、高效和直接的通信。RDMA提供了基于消息队列的点对点通信，每个应用都可以直接获取自己的消息，无需操作系统和协议栈的介入。 消息服务建立在通信双方本端和远端应用之间创建的Channel-IO连接之上。当应用需要通信时，就会创建一条Channel连接，每条Channel的首尾端点是两对Queue Pairs（QP），每对QP由Send Queue（SQ）和Receive Queue（RQ）构成，这些队列中管理着各种类型的消息。QP会被映射到应用的虚拟地址空间，使得应用直接通过它访问RNIC网卡。除了QP描述的两种基本队列之外，RDMA还提供一种队列Complete Queue（CQ），CQ用来知会用户WQ上的消息已经被处理完。 RDMA提供了一套软件传输接口，方便用户创建传输请求Work Request(WR），WR中描述了应用希望传输到Channel对端的消息内容，WR通知QP中的某个队列Work Queue（WQ）。在WQ中，用户的WR被转化为Work Queue Ellement（WQE）的格式，等待RNIC的异步调度解析，并从WQE指向的Buffer中拿到真正的消息发送到Channel对端。 RDMA中SEND/RECEIVE是双边操作，即必须要远端的应用感知参与才能完成收发。READ和WRITE是单边操作，只需要本端明确信息的源和目的地址，远端应用不必感知此次通信，数据的读或写都通过RDMA在RNIC与应用Buffer之间完成，再由远端RNIC封装成消息返回到本端。在实际中，SEND/RECEIVE多用于连接控制类报文，而数据报文多是通过READ/WRITE来完成的。 对于双边操作为例，主机A向主机B(下面简称A、B)发送数据的流程如下： 首先，A和B都要创建并初始化好各自的QP，CQ A和B分别向自己的WQ中注册WQE，对于A，WQ=SQ，WQE描述指向一个等到被发送的数据；对于B，WQ=RQ，WQE描述指向一块用于存储数据的Buffer。 A的RNIC异步调度轮到A的WQE，解析到这是一个SEND消息，从Buffer中直接向B发出数据。数据流到达B的RNIC后，B的WQE被消耗，并把数据直接存储到WQE指向的存储位置。 AB通信完成后，A的CQ中会产生一个完成消息CQE表示发送完成。与此同时，B的CQ中也会产生一个完成消息表示接收完成。每个WQ中WQE的处理完成都会产生一个CQE。双边操作与传统网络的底层Buffer Pool类似，收发双方的参与过程并无差别，区别在零拷贝、Kernel Bypass，实际上对于RDMA，这是一种复杂的消息传输模式，多用于传输短的控制消息。 对于单边操作，以存储网络环境下的存储为例(A作为文件系统，B作为存储介质)，数据的流程如下（RDMA READ）： 首先A、B建立连接，QP已经创建并且初始化。 数据被存档在A的buffer地址VA，注意VA应该提前注册到A的RNIC，并拿到返回的local key，相当于RDMA操作这块buffer的权限。 A把数据地址VA，key封装到专用的报文传送到B，这相当于A把数据buffer的操作权交给了B。同时A在它的WQ中注册进一个WR，以用于接收数据传输的B返回的状态。 B在收到A的送过来的数据VA和R_key后，RNIC会把它们连同存储地址VB到封装RDMA READ，这个过程A、B两端不需要任何软件参与，就可以将A的数据存储到B的VB虚拟地址。 B在存储完成后，会向A返回整个数据传输的状态信息。 单边操作传输方式是RDMA与传统网络传输的最大不同，只需提供直接访问远程的虚拟地址，无须远程应用的参与其中，这种方式适用于批量数据传输。 Infiniband的成功取决于两个因素，一是主机侧采用RDMA技术，可以把主机内数据处理的时延从几十微秒降低到几微秒，同时不占用CPU；二是InfiniBand网络的采用高带宽（40G/56G）、低时延（几百纳秒）和无丢包特性 随着以太网的发展，也具备高带宽和无丢包能力，在时延方面也能接近InfiniBand交换机的性能，所以RDMA over Ethernet（RoCE）成为必然，且RoCE组网成本更低。未来RoCE、iWARP和Infiniband等基于RDMA技术产品都会得到长足的发展。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[内存映射IO (MMIO)]]></title>
      <url>%2Fblog%2F2018%2F05%2F25%2F%E5%86%85%E5%AD%98%E6%98%A0%E5%B0%84IO-MMIO%2F</url>
      <content type="text"><![CDATA[MMIO(Memory mapping I/O)即内存映射I/O，它是PCI规范的一部分，I/O设备被放置在内存空间而不是I/O空间。从处理器的角度看，内存映射I/O后系统设备访问起来和内存一样。这样访问AGP/PCI-E显卡上的帧缓存，BIOS，PCI设备就可以使用读写内存一样的汇编指令完成，简化了程序设计的难度和接口的复杂性。 基本概念MMIO(Memory mapping I/O)即内存映射I/O，它是PCI规范的一部分，I/O设备被放置在内存空间而不是I/O空间。从处理器的角度看，内存映射I/O后系统设备访问起来和内存一样。这样访问AGP/PCI-E显卡上的帧缓存，BIOS，PCI设备就可以使用读写内存一样的汇编指令完成，简化了程序设计的难度和接口的复杂性。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O)。（来自百度百科）简而言之，MMIO就是通过将外围设备映射到内存空间，便于CPU的访问。I/O作为CPU和外设交流的一个渠道，主要分为两种，一种是Port I/O，一种是MMIO(Memory mapping I/O)。前者就是我们常说的I/O端口，它实际上的应该被称为I/O地址空间。小概念：32位操作系统，32bit的处理器，拥有32bit寻址能力，即可访问2^32=4G的物理地址，那么就具有4G内存的识别能力。物理地址：并不是指物理内存的地址，而是指处理器和系统内存之间所用到的地址，可以理解为CPU最为方便访问的地址（有别于我们之前所知道的物理地址的定义：段地址*16+偏移地址），而这一个内存并不独属于物理内存，而被分成了很多部分，物理内存当然也能够占用其中的一部分。 PortIO和MMIO 的主要区别1）前者不占用CPU的物理地址空间，后者占有（这是对x86架构说的，一些架构，如IA64，port I/O占用物理地址空间）。2）前者是顺序访问。也就是说在一条I/O指令完成前，下一条指令不会执行。例如通过Port I/O对设备发起了操作，造成了设备寄存器状态变化，这个变化在下一条指令执行前生效。uncache的MMIO通过uncahce memory的特性保证顺序性。3）使用方式不同由于port I/O有独立的64K I/O地址空间，但CPU的地址线只有一套，所以必须区分地址属于物理地址空间还是I/O地址空间。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PCIE和PCI 计算机总线]]></title>
      <url>%2Fblog%2F2018%2F05%2F23%2FPCIE%E5%92%8CPCI-%E8%AE%A1%E7%AE%97%E6%9C%BA%E6%80%BB%E7%BA%BF%2F</url>
      <content type="text"><![CDATA[PCIEPCI Express是新一代的总线接口。早在2001年的春季，英特尔公司就提出了要用新一代的技术取代PCI总线和多种芯片的内部连接，并称之为第三代I/O总线技术。随后在2001年底，包括Intel、AMD、DELL、IBM在内的20多家业界主导公司开始起草新技术的规范，并在2002年完成，对其正式命名为PCI Express。它采用了目前业内流行的点对点串行连接，比起PCI以及更早期的计算机总线的共享并行架构，每个设备都有自己的专用连接，不需要向整个总线请求带宽，而且可以把数据传输率提高到一个很高的频率，达到PCI所不能提供的高带宽。 基本概念PCI Express的接口根据总线位宽不同而有所差异，包括X1、X4、X8以及X16（X2模式将用于内部接口而非插槽模式）。较短的PCI Express卡可以插入较长的PCI Express插槽中使用。PCI Express接口能够支持热拔插，这也是个不小的飞跃。PCI Express卡支持的三种电压分别为+3.3V、3.3Vaux以及+12V。用于取代AGP接口的PCI Express接口位宽为X16，将能够提供5GB/s的带宽，即便有编码上的损耗但仍能够提供4GB/s左右的实际带宽，远远超过AGP 8X的2.1GB/s的带宽。 PCI Express规格从1条通道连接到32条通道连接，有非常强的伸缩性，以满足不同系统设备对数据传输带宽不同的需求。例如，PCI Express X1规格支持双向数据传输，每向数据传输带宽250MB/s，PCI Express X1已经可以满足主流声效芯片、网卡芯片和存储设备对数据传输带宽的需求，但是远远无法满足图形芯片对数据传输带宽的需求。因此，必须采用PCI Express X16，即16条点对点数据传输通道连接来取代传统的AGP总线。PCI Express X16也支持双向数据传输，每向数据传输带宽高达4GB/s，双向数据传输带宽有8GB/s之多，相比之下，广泛采用的AGP 8X数据传输只提供2.1GB/s的数据传输带宽。 尽管PCI Express技术规格允许实现X1（250MB/秒），X2，X4，X8，X12，X16和X32通道规格，但是依形式来看，PCI Express X1和PCI Express X16将成为PCI Express主流规格，同时芯片组厂商将在南桥芯片当中添加对PCI Express X1的支持，在北桥芯片当中添加对PCI Express X16的支持。除去提供极高数据传输带宽之外，PCI Express因为采用串行数据包方式传递数据，所以PCI Express接口每个针脚可以获得比传统I/O标准更多的带宽，这样就可以降低PCI Express设备生产成本和体积。另外，PCI Express也支持高阶电源管理，支持热插拔，支持数据同步传输，为优先传输数据进行带宽优化。 在兼容性方面，PCI Express在软件层面上兼容的PCI技术和设备，支持PCI设备和内存模组的初始化，也就是说驱动程序、操作系统无需推倒重来，就可以支持PCI Express设备。PCI Express是新一代能够提供大量带宽和丰富功能以实现令人激动的新式图形应用的全新架构。PCI Express可以为带宽渴求型应用分配相应的带宽，大幅提高中央处理器（CPU）和图形处理器（GPU）之间的带宽。对最终用户而言，他们可以感受影院级图象效果，并获得无缝多媒体体验。 PCI Express的主要优势就是数据传输速率高，目前最高的16X 2.0版本可达到10GB/s，而且还有相当大的发展潜力。PCI Express也有多种规格，从PCI Express 1X到PCI Express 16X，能满足一定时间内出现的低速设备和高速设备的需求。PCI-Express最新的接口是PCIe 3.0接口，其比特率为8GT/s，约为上一代产品带宽的两倍，并且包含发射器和接收器均衡、PLL改善以及时钟数据恢复等一系列重要的新功能，用以改善数据传输和数据保护性能。像INTEL、IBM、、LSI、OCZ、、三星(计划中)、SanDisk、STEC、SuperTalent和东芝(计划中)等，而针对海量的数据增长使得用户对规模更大、可扩展性更强的系统所应用，PCIe 3.0技术的加入最新的LSI MegaRAID控制器及HBA产品的出色性能，就可以实现更大的系统设计灵活性。 PCI Express采用串行方式传输Data。它和原有的ISA、PCI和AGP总线不同。这种传输方式，不必因为某个硬件的频率而影响到整个系统性能的发挥。当然了，整个系统依然是一个整体，但是我们可以方便的提高某一频率低的硬件的频率，以便系统在没有瓶颈的环境下使用。以串行方式提升频率增进效能，关键的限制在于采用什么样的物理传输介质。人们普遍采用铜线路，而理论上铜这个材质可以提供的传输极限是10 Gbps。这也就是为什么PCI Express的极限传输速度的答案。 因为PCI Express工作模式是一种称之为“电压差式传输”的方式。两条铜线，通过相互间的电压差来表示逻辑符号0和1。以这种方式进行资料传输，可以支持极高的运行频率。所以在速度达到10Gbps后，只需换用光纤（Fibre Channel）就可以使之效能倍增。 PCI Express是下一阶段的主要传输总线带宽技术。然而，GPU对总线带宽的需求是子系统中最高的，显而易见的是，视频在PCI Express应占有一定的分量。显然，PCI Express的提出，并非是总线形式的一个结束。恰恰相反，其技术的成熟仍旧需要这个时间。当然了，趁这个时间，那些芯片、主板、视频等厂家是否能出来支持是PCI Express发展的关键。 PCI-Express是最新的总线和接口标准，它原来的名称为“3GIO”，是由英特尔提出的，很明显英特尔的意思是它代表着下一代I/O接口标准。交由PCI-SIG（PCI特殊兴趣组织）认证发布后才改名为“PCI-Express”。这个新标准将全面取代现行的PCI和AGP，最终实现总线标准的统一。它的主要优势就是数据传输速率高，目前最高可达到10GB/s以上，而且还有相当大的发展潜力。PCI Express也有多种规格，从PCI Express 1X到PCI Express 16X，芯片组。当然要实现全面取代PCI和AGP也需要一个相当长的过程，就象当初PCI取代ISA一样，都会有个过渡的过程。 PCI和PCIE有什么区别 在兼容性方面，PCI-E在软件层面上兼容目前的PCI技术和设备，支持PCI设备和内存模组的初始。 由于PCI总线只有133MB/s 的带宽，对声卡、网卡、视频卡等绝大多数输入/输出设备显得绰绰有余，但对性能日益强大的显卡则无法满足其需求。 目前PCI接口的显卡已经不多见了，只有较老的PC上才有，厂商也很少推出此类接口的产品。PCI显卡性能受到极大限制，并且由于数量稀少，因此价格也并不便宜，只有在不得已的情况才考虑使用PCI显卡。 因此，用于取代AGP接口的PCI-E接口位宽为X16，能够提供5GB/s的带宽，即便有编码上的损耗但仍能够提供约为4GB/s左右的实际带宽，远远超过AGP 8X的2.1GB/s的带宽。 PCI(Peripheral Component Interconnect)外部设备互连总线是 英特尔（Intel）公司1991年下半年首先提出的，并得到IBM、Compad、AST、HP、和DEC等100多家计算机公司的响应，于1993年正式推出了PCI局部总线标准。此标准允许在计算机内安装多达10个遵从PCI标准的扩展卡。 PCI-Express是最新的总线和接口标准，它原来的名称为“3GIO”，是由英特尔提出的，很明显英特尔的意思是它代表着下一代I/O接口标准。交由PCI-SIG（PCI特殊兴趣组织）认证发布后才改名为“PCI-Express”，简称“PCI-E”。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Design Guidelines for High Performance RDMA Systems]]></title>
      <url>%2Fblog%2F2018%2F05%2F23%2FDesign-Guidelines-for-High-Performance-RDMA-Systems%2F</url>
      <content type="text"><![CDATA[Introduction随着RDMA技术的发展，RDMA技术越来越被数据中心采用。尽管RDMA的新知名度，使用他们的先进功能以达到最佳效果仍然是软件设计师的挑战。找到RDMA功能与应用程序之间的有效匹配非常重要。没有一种方法能够适合所有的应用场景，比如说RDMA一个参数的最佳和最差选择在它们的总吞吐量中变化了70倍，并且它们消耗的主机CPU的量变化了3.2倍。在不同的设计中，应用需求的小的变化显著影响RDMA的相对性能。 首先这篇文章的第一个贡献就是它提供了由一组开源的度量工具支持的指导方针，用于评估和优化在使用RDMA NICs时影响端到端吞吐量的最重要的系统因素。对于每个指导方针。作者就如何确定本指南是否相关提供了深入的见解，并讨论了使用NIC的哪些模式最有可能缓解问题。 其次，作者通过第三代RDMA硬件将这些指南应用于微基准和实际系统来评估这些指南的功效。 Background图显示了RDMA Cluster硬件组件。其中NIC网卡连接的一个或则多个端口连接到PCIe控制器且连接到多核CPU的服务器上。PCIe恐控制器用来接受NIC网卡的PCIe请求到L3 cache中。在现代的Intel 服务器架构中，这个L3 Cache 提供PCIe 事件控制器。 PCI Express]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2Fblog%2F2017%2F09%2F25%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux服务管理]]></title>
      <url>%2Fblog%2F2017%2F04%2F25%2FLinux%E6%9C%8D%E5%8A%A1%E7%AE%A1%E7%90%86%2F</url>
      <content type="text"><![CDATA[Linux服务器管理第一章简介与分类1-1系统运行级别 0关机 1单用户模式 2不完全的命令行模式 3完全的命令行模式 4系统保留 5图像模式 6重启动 修改系统默认允许级别方法 vim /etc/inittab系统开机后直接进入哪个运行级别 1-2服务的分类 rpm包默认安装的服务 独立的服务 基于xinetd的服务 源码包安装的服务 1-3启动与自启动 服务启动：就是在当前系统中让服务运行，并且提供功能 服务自启动：自启动就是指在系统开机或者重启动以后，随着系统启动而自启动服务。 1-4查看系统中启动的服务 RPM包安装的服务 chkconfig –list 查看服务启动状态，可以看到所有RPM包安装的服务.RPM包安装位置在/etc/init/init.d 目录下 源码包安装的服务 查看服务安装的位置一般在 /usr/local 目录下 1-5 服务和端口 端口和服务相对应 查询系统中开启的服务 netstat -tlunp 第二章RPM包服务管理]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo 强大的博客框架]]></title>
      <url>%2Fblog%2F2017%2F04%2F25%2FHexo-%E5%BC%BA%E5%A4%A7%E7%9A%84%E5%8D%9A%E5%AE%A2%E6%A1%86%E6%9E%B6%2F</url>
      <content type="text"><![CDATA[正文: 什么是Hexo Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装 安装 Hexo 只需几分钟时间，若您在安装过程中遇到问题或无法找到解决方式，请提交问题，我会尽力解决您的问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[hello]]></title>
      <url>%2Fblog%2F2015%2F07%2F01%2Fhello-blog%2F</url>
      <content type="text"><![CDATA[摘要:正文:]]></content>
    </entry>

    
  
  
</search>
